{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ad02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CombinedDataReader:\n",
    "    \"\"\"\n",
    "    A class to read and process combined machine data JSON files into structured DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_directory: str):\n",
    "        \"\"\"\n",
    "        Initialize the data reader with the directory containing combined JSON files.\n",
    "        \n",
    "        Args:\n",
    "            data_directory (str): Path to the directory containing combined JSON files\n",
    "        \"\"\"\n",
    "        self.data_directory = data_directory\n",
    "        self.metadata_df = None\n",
    "        self.conditions_df = None\n",
    "        self.samples_df = None\n",
    "        self.events_df = None\n",
    "        self.machines_df = None\n",
    "        self.components_df = None\n",
    "        \n",
    "    def get_json_files(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get all JSON files from the combined data directory.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of JSON file paths\n",
    "        \"\"\"\n",
    "        pattern = os.path.join(self.data_directory, \"*.json\")\n",
    "        json_files = glob.glob(pattern)\n",
    "        logger.info(f\"Found {len(json_files)} JSON files in {self.data_directory}\")\n",
    "        return json_files\n",
    "    \n",
    "    def parse_machine_info(self, filename: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Parse machine information from filename.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): The JSON filename\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[str, str, str]: machine_name, data_type, timestamp\n",
    "        \"\"\"\n",
    "        basename = os.path.basename(filename)\n",
    "        # Expected format: mazak_1_vtc_200_current_merged_20250707_152414.json\n",
    "        parts = basename.replace('.json', '').split('_')\n",
    "        \n",
    "        if 'current' in basename:\n",
    "            machine_name = '_'.join(parts[:-4])  # mazak_1_vtc_200\n",
    "            data_type = 'current'\n",
    "            timestamp_part = parts[-2] + '_' + parts[-1]  # 20250707_152414\n",
    "        elif 'sample' in basename:\n",
    "            machine_name = '_'.join(parts[:-4])  # mazak_1_vtc_200\n",
    "            data_type = 'sample'\n",
    "            timestamp_part = parts[-2] + '_' + parts[-1]  # 20250707_152414\n",
    "        else:\n",
    "            # Fallback parsing\n",
    "            machine_name = '_'.join(parts[:-3])\n",
    "            data_type = parts[-3]\n",
    "            timestamp_part = parts[-2] + '_' + parts[-1]\n",
    "            \n",
    "        return machine_name, data_type, timestamp_part\n",
    "    \n",
    "    def read_single_file(self, filepath: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Read and parse a single JSON file.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to the JSON file\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Parsed JSON data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            logger.info(f\"Successfully loaded {os.path.basename(filepath)}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {filepath}: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def process_metadata(self, json_data: Dict, machine_name: str, data_type: str, \n",
    "                        filename: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process metadata section of the JSON file.\n",
    "        \n",
    "        Args:\n",
    "            json_data (Dict): The parsed JSON data\n",
    "            machine_name (str): Machine identifier\n",
    "            data_type (str): Type of data (current/sample)\n",
    "            filename (str): Original filename\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Processed metadata\n",
    "        \"\"\"\n",
    "        metadata = json_data.get('metadata', {})\n",
    "        \n",
    "        processed_metadata = {\n",
    "            'file_name': os.path.basename(filename),\n",
    "            'machine_name': machine_name,\n",
    "            'data_type': data_type,\n",
    "            'created_at': metadata.get('created_at'),\n",
    "            'machine_id': metadata.get('machine_id'),\n",
    "            'total_json_files': metadata.get('total_json_files', 0),\n",
    "            'total_xml_files': metadata.get('total_xml_files', 0),\n",
    "            'total_data_sources': len(metadata.get('data_sources', [])),\n",
    "        }\n",
    "        \n",
    "        return processed_metadata\n",
    "    \n",
    "    def process_conditions(self, json_data: Dict, machine_name: str, data_type: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process conditions data from components.\n",
    "        \n",
    "        Args:\n",
    "            json_data (Dict): The parsed JSON data\n",
    "            machine_name (str): Machine identifier\n",
    "            data_type (str): Type of data (current/sample)\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of condition records\n",
    "        \"\"\"\n",
    "        conditions = []\n",
    "        device = json_data.get('data', {}).get('device', {})\n",
    "        components = device.get('components', {})\n",
    "        \n",
    "        for component_id, component_data in components.items():\n",
    "            component_type = component_data.get('type', '')\n",
    "            component_name = component_data.get('name', '')\n",
    "            component_conditions = component_data.get('conditions', {})\n",
    "            \n",
    "            for condition_name, condition_data in component_conditions.items():\n",
    "                condition_record = {\n",
    "                    'machine_name': machine_name,\n",
    "                    'data_type': data_type,\n",
    "                    'component_id': component_id,\n",
    "                    'component_type': component_type,\n",
    "                    'component_name': component_name,\n",
    "                    'condition_name': condition_name,\n",
    "                    'timestamp': condition_data.get('timestamp'),\n",
    "                    'sequence': condition_data.get('sequence'),\n",
    "                    'state': condition_data.get('state'),\n",
    "                    'category': condition_data.get('category'),\n",
    "                }\n",
    "                conditions.append(condition_record)\n",
    "        \n",
    "        return conditions\n",
    "    \n",
    "    def process_samples(self, json_data: Dict, machine_name: str, data_type: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process samples data from components.\n",
    "        \n",
    "        Args:\n",
    "            json_data (Dict): The parsed JSON data\n",
    "            machine_name (str): Machine identifier\n",
    "            data_type (str): Type of data (current/sample)\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of sample records\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        device = json_data.get('data', {}).get('device', {})\n",
    "        components = device.get('components', {})\n",
    "        \n",
    "        for component_id, component_data in components.items():\n",
    "            component_type = component_data.get('type', '')\n",
    "            component_name = component_data.get('name', '')\n",
    "            component_samples = component_data.get('samples', {})\n",
    "            \n",
    "            for sample_name, sample_list in component_samples.items():\n",
    "                if isinstance(sample_list, list):\n",
    "                    for sample_data in sample_list:\n",
    "                        sample_record = {\n",
    "                            'machine_name': machine_name,\n",
    "                            'data_type': data_type,\n",
    "                            'component_id': component_id,\n",
    "                            'component_type': component_type,\n",
    "                            'component_name': component_name,\n",
    "                            'sample_name': sample_name,\n",
    "                            'timestamp': sample_data.get('timestamp'),\n",
    "                            'sequence': sample_data.get('sequence'),\n",
    "                            'value': sample_data.get('value'),\n",
    "                            'sub_type': sample_data.get('subType'),\n",
    "                        }\n",
    "                        samples.append(sample_record)\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def process_events(self, json_data: Dict, machine_name: str, data_type: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process events data from components.\n",
    "        \n",
    "        Args:\n",
    "            json_data (Dict): The parsed JSON data\n",
    "            machine_name (str): Machine identifier\n",
    "            data_type (str): Type of data (current/sample)\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of event records\n",
    "        \"\"\"\n",
    "        events = []\n",
    "        device = json_data.get('data', {}).get('device', {})\n",
    "        components = device.get('components', {})\n",
    "        \n",
    "        for component_id, component_data in components.items():\n",
    "            component_type = component_data.get('type', '')\n",
    "            component_name = component_data.get('name', '')\n",
    "            component_events = component_data.get('events', {})\n",
    "            \n",
    "            for event_name, event_list in component_events.items():\n",
    "                if isinstance(event_list, list):\n",
    "                    for event_data in event_list:\n",
    "                        event_record = {\n",
    "                            'machine_name': machine_name,\n",
    "                            'data_type': data_type,\n",
    "                            'component_id': component_id,\n",
    "                            'component_type': component_type,\n",
    "                            'component_name': component_name,\n",
    "                            'event_name': event_name,\n",
    "                            'timestamp': event_data.get('timestamp'),\n",
    "                            'sequence': event_data.get('sequence'),\n",
    "                            'value': event_data.get('value'),\n",
    "                        }\n",
    "                        events.append(event_record)\n",
    "        \n",
    "        return events\n",
    "    \n",
    "    def process_components(self, json_data: Dict, machine_name: str, data_type: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process component information.\n",
    "        \n",
    "        Args:\n",
    "            json_data (Dict): The parsed JSON data\n",
    "            machine_name (str): Machine identifier\n",
    "            data_type (str): Type of data (current/sample)\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of component records\n",
    "        \"\"\"\n",
    "        components = []\n",
    "        device = json_data.get('data', {}).get('device', {})\n",
    "        device_components = device.get('components', {})\n",
    "        \n",
    "        for component_id, component_data in device_components.items():\n",
    "            component_record = {\n",
    "                'machine_name': machine_name,\n",
    "                'data_type': data_type,\n",
    "                'component_id': component_id,\n",
    "                'component_type': component_data.get('type', ''),\n",
    "                'component_name': component_data.get('name', ''),\n",
    "                'has_conditions': bool(component_data.get('conditions', {})),\n",
    "                'has_samples': bool(component_data.get('samples', {})),\n",
    "                'has_events': bool(component_data.get('events', {})),\n",
    "                'conditions_count': len(component_data.get('conditions', {})),\n",
    "                'samples_count': sum(len(v) if isinstance(v, list) else 0 \n",
    "                                   for v in component_data.get('samples', {}).values()),\n",
    "                'events_count': sum(len(v) if isinstance(v, list) else 0 \n",
    "                                  for v in component_data.get('events', {}).values()),\n",
    "            }\n",
    "            components.append(component_record)\n",
    "        \n",
    "        return components\n",
    "    \n",
    "    def process_machines(self, json_data: Dict, machine_name: str, data_type: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process machine-level information.\n",
    "        \n",
    "        Args:\n",
    "            json_data (Dict): The parsed JSON data\n",
    "            machine_name (str): Machine identifier\n",
    "            data_type (str): Type of data (current/sample)\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Machine record\n",
    "\n",
    "        \"\"\"\n",
    "        device = json_data.get('data', {}).get('device', {})\n",
    "        \n",
    "        machine_record = {\n",
    "            'machine_name': machine_name,\n",
    "            'data_type': data_type,\n",
    "            'device_name': device.get('name', ''),\n",
    "            'device_uuid': device.get('uuid', ''),\n",
    "            'components_count': len(device.get('components', {})),\n",
    "        }\n",
    "        \n",
    "        return machine_record\n",
    "    \n",
    "    def read_all_files(self) -> None:\n",
    "        \"\"\"\n",
    "        Read all JSON files and process them into DataFrames.\n",
    "        \"\"\"\n",
    "        json_files = self.get_json_files()\n",
    "        \n",
    "        all_metadata = []\n",
    "        all_conditions = []\n",
    "        all_samples = []\n",
    "        all_events = []\n",
    "        all_components = []\n",
    "        all_machines = []\n",
    "        \n",
    "        for filepath in json_files:\n",
    "            logger.info(f\"Processing {os.path.basename(filepath)}\")\n",
    "            \n",
    "            # Parse machine info from filename\n",
    "            machine_name, data_type, timestamp = self.parse_machine_info(filepath)\n",
    "            \n",
    "            # Read JSON data\n",
    "            json_data = self.read_single_file(filepath)\n",
    "            if not json_data:\n",
    "                continue\n",
    "            \n",
    "            # Process each data type\n",
    "            metadata = self.process_metadata(json_data, machine_name, data_type, filepath)\n",
    "            all_metadata.append(metadata)\n",
    "            \n",
    "            conditions = self.process_conditions(json_data, machine_name, data_type)\n",
    "            all_conditions.extend(conditions)\n",
    "            \n",
    "            samples = self.process_samples(json_data, machine_name, data_type)\n",
    "            all_samples.extend(samples)\n",
    "            \n",
    "            events = self.process_events(json_data, machine_name, data_type)\n",
    "            all_events.extend(events)\n",
    "            \n",
    "            components = self.process_components(json_data, machine_name, data_type)\n",
    "            all_components.extend(components)\n",
    "            \n",
    "            machine = self.process_machines(json_data, machine_name, data_type)\n",
    "            all_machines.append(machine)\n",
    "        \n",
    "        # Create DataFrames\n",
    "        logger.info(\"Creating DataFrames...\")\n",
    "        \n",
    "        self.metadata_df = pd.DataFrame(all_metadata)\n",
    "        self.conditions_df = pd.DataFrame(all_conditions)\n",
    "        self.samples_df = pd.DataFrame(all_samples)\n",
    "        self.events_df = pd.DataFrame(all_events)\n",
    "        self.components_df = pd.DataFrame(all_components)\n",
    "        self.machines_df = pd.DataFrame(all_machines)\n",
    "        \n",
    "        # Convert timestamp columns\n",
    "        self._convert_timestamps()\n",
    "        \n",
    "        logger.info(\"DataFrames created successfully!\")\n",
    "        self._print_summary()\n",
    "    \n",
    "    def _convert_timestamps(self) -> None:\n",
    "        \"\"\"Convert timestamp columns to proper datetime format.\"\"\"\n",
    "        timestamp_columns = {\n",
    "            'metadata_df': ['created_at'],\n",
    "            'conditions_df': ['timestamp'],\n",
    "            'samples_df': ['timestamp'],\n",
    "            'events_df': ['timestamp'],\n",
    "        }\n",
    "        \n",
    "        for df_name, columns in timestamp_columns.items():\n",
    "            df = getattr(self, df_name)\n",
    "            if df is not None:\n",
    "                for col in columns:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    def _prepare_for_parquet(self) -> None:\n",
    "        \"\"\"Prepare DataFrames for Parquet format by ensuring consistent data types.\"\"\"\n",
    "        # Convert 'value' columns to string to handle mixed types\n",
    "        value_columns = {\n",
    "            'samples_df': ['value'],\n",
    "            'events_df': ['value'],\n",
    "        }\n",
    "        \n",
    "        for df_name, columns in value_columns.items():\n",
    "            df = getattr(self, df_name)\n",
    "            if df is not None:\n",
    "                for col in columns:\n",
    "                    if col in df.columns:\n",
    "                        # Convert to string, handling None/null values\n",
    "                        df[col] = df[col].astype(str)\n",
    "                        # Replace 'nan' strings with None for proper null handling\n",
    "                        df[col] = df[col].replace('nan', None)\n",
    "    \n",
    "    def _print_summary(self) -> None:\n",
    "        \"\"\"Print summary of loaded data.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA LOADING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        dataframes = {\n",
    "            'Metadata': self.metadata_df,\n",
    "            'Machines': self.machines_df,\n",
    "            'Components': self.components_df,\n",
    "            'Conditions': self.conditions_df,\n",
    "            'Samples': self.samples_df,\n",
    "            'Events': self.events_df,\n",
    "        }\n",
    "        \n",
    "        for name, df in dataframes.items():\n",
    "            if df is not None:\n",
    "                print(f\"{name:12}: {len(df):8,} records\")\n",
    "            else:\n",
    "                print(f\"{name:12}: No data\")\n",
    "        \n",
    "        print(\"\\nMachine Summary:\")\n",
    "        if self.machines_df is not None:\n",
    "            machine_summary = self.machines_df.groupby('machine_name')['data_type'].apply(list).to_dict()\n",
    "            for machine, types in machine_summary.items():\n",
    "                print(f\"  {machine}: {', '.join(types)}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def get_dataframes(self) -> Dict[str, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Get all processed DataFrames.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, Optional[pd.DataFrame]]: Dictionary of DataFrames\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'metadata': self.metadata_df,\n",
    "            'machines': self.machines_df,\n",
    "            'components': self.components_df,\n",
    "            'conditions': self.conditions_df,\n",
    "            'samples': self.samples_df,\n",
    "            'events': self.events_df,\n",
    "        }\n",
    "    \n",
    "    def save_to_csv(self, output_directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Save all DataFrames to CSV files.\n",
    "        \n",
    "        Args:\n",
    "            output_directory (str): Directory to save CSV files\n",
    "        \"\"\"\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        \n",
    "        dataframes = self.get_dataframes()\n",
    "        \n",
    "        for name, df in dataframes.items():\n",
    "            if df is not None and not df.empty:\n",
    "                output_path = os.path.join(output_directory, f\"{name}.csv\")\n",
    "                df.to_csv(output_path, index=False)\n",
    "                logger.info(f\"Saved {name} to {output_path}\")\n",
    "        \n",
    "        logger.info(f\"All DataFrames saved to {output_directory}\")\n",
    "    \n",
    "    def save_to_parquet(self, output_directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Save all DataFrames to Parquet files for better performance.\n",
    "        \n",
    "        Args:\n",
    "            output_directory (str): Directory to save Parquet files\n",
    "        \"\"\"\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        \n",
    "        # Prepare data types for Parquet format\n",
    "        self._prepare_for_parquet()\n",
    "        \n",
    "        dataframes = self.get_dataframes()\n",
    "        \n",
    "        for name, df in dataframes.items():\n",
    "            if df is not None and not df.empty:\n",
    "                output_path = os.path.join(output_directory, f\"{name}.parquet\")\n",
    "                df.to_parquet(output_path, index=False)\n",
    "                logger.info(f\"Saved {name} to {output_path}\")\n",
    "        \n",
    "        logger.info(f\"All DataFrames saved to {output_directory}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate usage of the CombinedDataReader.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    data_directory = \"/Users/tingxu/Local/observability/src/data/combined\"\n",
    "    output_directory = \"/Users/tingxu/Local/observability/src/data/processed\"\n",
    "    \n",
    "    # Initialize reader\n",
    "    reader = CombinedDataReader(data_directory)\n",
    "    \n",
    "    # Read all files\n",
    "    reader.read_all_files()\n",
    "    \n",
    "    # Get DataFrames for further processing\n",
    "    dataframes = reader.get_dataframes()\n",
    "    \n",
    "    # Save to CSV and Parquet\n",
    "    reader.save_to_csv(output_directory)\n",
    "    reader.save_to_parquet(output_directory)\n",
    "    \n",
    "    # Example: Display first few rows of each DataFrame\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAMPLE DATA PREVIEW\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for name, df in dataframes.items():\n",
    "        if df is not None and not df.empty:\n",
    "            print(f\"{name.upper()} (first 3 rows):\")\n",
    "            print(df.head(3).to_string())\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "938bf89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 15:48:21,062 - INFO - Found 8 JSON files in /Users/tingxu/Local/observability/src/data/combined\n",
      "2025-07-07 15:48:21,063 - INFO - Processing mazak_1_vtc_200_sample_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,066 - INFO - Successfully loaded mazak_1_vtc_200_sample_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,067 - INFO - Processing mazak_3_350msy_current_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,071 - INFO - Successfully loaded mazak_3_350msy_current_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,073 - INFO - Processing mazak_2_vtc_300_sample_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,075 - INFO - Successfully loaded mazak_2_vtc_300_sample_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,078 - INFO - Processing mazak_2_vtc_300_current_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,085 - INFO - Successfully loaded mazak_2_vtc_300_current_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,086 - INFO - Processing mazak_4_vtc_300c_sample_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,087 - INFO - Successfully loaded mazak_4_vtc_300c_sample_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,089 - INFO - Processing mazak_3_350msy_sample_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,091 - INFO - Successfully loaded mazak_3_350msy_sample_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,092 - INFO - Processing mazak_1_vtc_200_current_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,094 - INFO - Successfully loaded mazak_1_vtc_200_current_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,096 - INFO - Processing mazak_4_vtc_300c_current_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,099 - INFO - Successfully loaded mazak_4_vtc_300c_current_merged_20250707_152414.json\n",
      "2025-07-07 15:48:21,100 - INFO - Creating DataFrames...\n",
      "2025-07-07 15:48:21,121 - INFO - DataFrames created successfully!\n",
      "2025-07-07 15:48:21,132 - INFO - Saved metadata to /Users/tingxu/Local/observability/src/data/processed/metadata.csv\n",
      "2025-07-07 15:48:21,134 - INFO - Saved machines to /Users/tingxu/Local/observability/src/data/processed/machines.csv\n",
      "2025-07-07 15:48:21,136 - INFO - Saved components to /Users/tingxu/Local/observability/src/data/processed/components.csv\n",
      "2025-07-07 15:48:21,138 - INFO - Saved conditions to /Users/tingxu/Local/observability/src/data/processed/conditions.csv\n",
      "2025-07-07 15:48:21,146 - INFO - Saved samples to /Users/tingxu/Local/observability/src/data/processed/samples.csv\n",
      "2025-07-07 15:48:21,157 - INFO - Saved events to /Users/tingxu/Local/observability/src/data/processed/events.csv\n",
      "2025-07-07 15:48:21,157 - INFO - All DataFrames saved to /Users/tingxu/Local/observability/src/data/processed\n",
      "2025-07-07 15:48:21,170 - INFO - Saved metadata to /Users/tingxu/Local/observability/src/data/processed/metadata.parquet\n",
      "2025-07-07 15:48:21,172 - INFO - Saved machines to /Users/tingxu/Local/observability/src/data/processed/machines.parquet\n",
      "2025-07-07 15:48:21,175 - INFO - Saved components to /Users/tingxu/Local/observability/src/data/processed/components.parquet\n",
      "2025-07-07 15:48:21,178 - INFO - Saved conditions to /Users/tingxu/Local/observability/src/data/processed/conditions.parquet\n",
      "2025-07-07 15:48:21,183 - INFO - Saved samples to /Users/tingxu/Local/observability/src/data/processed/samples.parquet\n",
      "2025-07-07 15:48:21,191 - INFO - Saved events to /Users/tingxu/Local/observability/src/data/processed/events.parquet\n",
      "2025-07-07 15:48:21,191 - INFO - All DataFrames saved to /Users/tingxu/Local/observability/src/data/processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA LOADING SUMMARY\n",
      "============================================================\n",
      "Metadata    :        8 records\n",
      "Machines    :        8 records\n",
      "Components  :       73 records\n",
      "Conditions  :       57 records\n",
      "Samples     :      995 records\n",
      "Events      :    1,004 records\n",
      "\n",
      "Machine Summary:\n",
      "  mazak_1_vtc_200: sample, current\n",
      "  mazak_2_vtc_300: sample, current\n",
      "  mazak_3_350msy: current, sample\n",
      "  mazak_4_vtc_300c: sample, current\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SAMPLE DATA PREVIEW\n",
      "============================================================\n",
      "METADATA (first 3 rows):\n",
      "                                            file_name     machine_name data_type                 created_at              machine_id  total_json_files  total_xml_files  total_data_sources\n",
      "0  mazak_1_vtc_200_sample_merged_20250707_152414.json  mazak_1_vtc_200    sample 2025-07-07 15:24:14.908662  mazak-1-vtc-200_sample                 7                7                  14\n",
      "1  mazak_3_350msy_current_merged_20250707_152414.json   mazak_3_350msy   current 2025-07-07 15:24:14.882793  mazak-3-350msy_current                 7                7                  14\n",
      "2  mazak_2_vtc_300_sample_merged_20250707_152414.json  mazak_2_vtc_300    sample 2025-07-07 15:24:14.896665  mazak-2-vtc-300_sample                 8                8                  16\n",
      "MACHINES (first 3 rows):\n",
      "      machine_name data_type device_name  device_uuid  components_count\n",
      "0  mazak_1_vtc_200    sample       Mazak        Mazak                 6\n",
      "1   mazak_3_350msy   current       Mazak  M8011CS193N                18\n",
      "2  mazak_2_vtc_300    sample       Mazak        Mazak                 6\n",
      "COMPONENTS (first 3 rows):\n",
      "      machine_name data_type component_id component_type component_name  has_conditions  has_samples  has_events  conditions_count  samples_count  events_count\n",
      "0  mazak_1_vtc_200    sample            c         Rotary              C           False         True       False                 0             12             0\n",
      "1  mazak_1_vtc_200    sample         cont     Controller     controller           False         True       False                 0             14             0\n",
      "2  mazak_1_vtc_200    sample        path1           Path           path           False         True        True                 0              7            26\n",
      "CONDITIONS (first 3 rows):\n",
      "     machine_name data_type component_id component_type component_name condition_name                        timestamp  sequence  state  category\n",
      "0  mazak_3_350msy   current            a           Axes           base     servo_cond 2025-07-07 13:07:32.515233+00:00  16600039  #text  ACTUATOR\n",
      "1  mazak_3_350msy   current            a           Axes           base   spindle_cond 2025-07-07 13:07:32.515233+00:00  16600043  #text    SYSTEM\n",
      "2  mazak_3_350msy   current           ar         Rotary              B        Btravel 2025-07-07 13:07:32.515233+00:00  16599978  #text     ANGLE\n",
      "SAMPLES (first 3 rows):\n",
      "      machine_name data_type component_id component_type component_name sample_name                        timestamp  sequence value sub_type\n",
      "0  mazak_1_vtc_200    sample            c         Rotary              C       Sload 2025-07-07 17:28:17.557398+00:00   6634144   1.0     None\n",
      "1  mazak_1_vtc_200    sample            c         Rotary              C       Sload 2025-07-07 17:28:17.557398+00:00   6634144   1.0     None\n",
      "2  mazak_1_vtc_200    sample            c         Rotary              C       Sload 2025-07-07 17:28:03.705129+00:00   6634087   4.0     None\n",
      "EVENTS (first 3 rows):\n",
      "      machine_name data_type component_id component_type component_name   event_name                        timestamp  sequence value\n",
      "0  mazak_1_vtc_200    sample        path1           Path           path  Tool_number 2025-07-07 17:28:15.253528+00:00   6634133    45\n",
      "1  mazak_1_vtc_200    sample        path1           Path           path  Tool_number 2025-07-07 17:28:15.253528+00:00   6634133    45\n",
      "2  mazak_1_vtc_200    sample        path1           Path           path  Tool_number 2025-07-07 17:27:47.520931+00:00   6634023    33\n"
     ]
    }
   ],
   "source": [
    "dataframes= main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ba485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-20.0.0-cp312-cp312-macosx_12_0_arm64.whl (30.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-20.0.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7dad92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
